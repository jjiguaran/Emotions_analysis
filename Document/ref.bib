@article{pang2008opinion,
	title={Opinion mining and sentiment analysis},
	author={Pang, Bo and Lee, Lillian and others},
	journal={Foundations and Trends{\textregistered} in information retrieval},
	volume={2},
	number={1--2},
	pages={1--135},
	year={2008},
	publisher={Now Publishers, Inc.}
}

@inproceedings{pak2010twitter,
	title={Twitter as a corpus for sentiment analysis and opinion mining},
	author={Pak, Alexander and Paroubek, Patrick},
	booktitle={Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10)},
	year={2010}
}

@inproceedings{alm2005emotions,
	title={Emotions from text: machine learning for text-based emotion prediction},
	author={Alm, Cecilia Ovesdotter and Roth, Dan and Sproat, Richard},
	booktitle={Proceedings of human language technology conference and conference on empirical methods in natural language processing},
	pages={579--586},
	year={2005}
}

@inproceedings{roberts2012empatweet,
	title={Empatweet: Annotating and detecting emotions on twitter},
	author={Roberts, Kirk and Roach, Michael A and Johnson, Joseph and Guthrie, Josh and Harabagiu, Sanda},
	booktitle={Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)},
	pages={3806--3813},
	year={2012}
}

@article{ekman1993facial,
	title={Facial expression and emotion.},
	author={Ekman, Paul},
	journal={American psychologist},
	volume={48},
	number={4},
	pages={384},
	year={1993},
	publisher={American Psychological Association}
}

@book{picard2000affective,
	title={Affective computing},
	author={Picard, Rosalind W},
	year={2000},
	publisher={MIT press}
}



@article{go2009twitter,
	title={Twitter sentiment classification using distant supervision},
	author={Go, Alec and Bhayani, Richa and Huang, Lei},
	journal={CS224N project report, Stanford},
	volume={1},
	number={12},
	pages={2009},
	year={2009}
}

@inproceedings{bollen2011modeling,
	title={Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena},
	author={Bollen, Johan and Mao, Huina and Pepe, Alberto},
	booktitle={Proceedings of the international AAAI conference on web and social media},
	volume={5},
	number={1},
	pages={450--453},
	year={2011}
}




@inproceedings{kouloumpis2011twitter,
	title={Twitter sentiment analysis: The good the bad and the omg!},
	author={Kouloumpis, Efthymios and Wilson, Theresa and Moore, Johanna},
	booktitle={Proceedings of the international AAAI conference on web and social media},
	volume={5},
	number={1},
	pages={538--541},
	year={2011}
}

@article{pang2002thumbs,
	title={Thumbs up? Sentiment classification using machine learning techniques},
	author={Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
	journal={arXiv preprint cs/0205070},
	year={2002}
}





@inproceedings{wang2012harnessing,
	title={Harnessing twitter" big data" for automatic emotion identification},
	author={Wang, Wenbo and Chen, Lu and Thirunarayan, Krishnaprasad and Sheth, Amit P},
	booktitle={2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing},
	pages={587--592},
	year={2012},
	organization={IEEE}
}



@inproceedings{mohammad2012emotional,
	title={\# Emotional tweets},
	author={Mohammad, Saif},
	booktitle={* SEM 2012: The First Joint Conference on Lexical and Computational Semantics--Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012)},
	pages={246--255},
	year={2012}
}

@inproceedings{aman2007identifying,
	title={Identifying expressions of emotion in text},
	author={Aman, Saima and Szpakowicz, Stan},
	booktitle={International Conference on Text, Speech and Dialogue},
	pages={196--205},
	year={2007},
	organization={Springer}
}

@inproceedings{tumasjan2010predicting,
	title={Predicting elections with twitter: What 140 characters reveal about political sentiment},
	author={Tumasjan, Andranik and Sprenger, Timm and Sandner, Philipp and Welpe, Isabell},
	booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
	volume={4},
	number={1},
	pages={178--185},
	year={2010}
}

@article{ortony1987referential,
	title={hatzivassiloglou1997predicting
	},
	author={Ortony, Andrew and Clore, Gerald L and Foss, Mark A},
	journal={Cognitive science},
	volume={11},
	number={3},
	pages={341--364},
	year={1987},
	publisher={Wiley Online Library}
}

@inproceedings{strapparava2004wordnet,
	title={Wordnet affect: an affective extension of wordnet.},
	author={Strapparava, Carlo and Valitutti, Alessandro and others},
	booktitle={Lrec},
	volume={4},
	number={1083-1086},
	pages={40},
	year={2004},
	organization={Lisbon, Portugal}
}

@article{wiebe2005annotating,
	title={Annotating expressions of opinions and emotions in language},
	author={Wiebe, Janyce and Wilson, Theresa and Cardie, Claire},
	journal={Language resources and evaluation},
	volume={39},
	number={2},
	pages={165--210},
	year={2005},
	publisher={Springer}
}

@inproceedings{ceron2016sentiment,
	title={A sentiment analysis system of Spanish tweets and its application in Colombia 2014 presidential election},
	author={Cer{\'o}n-Guzm{\'a}n, Jhon Adri{\'a}n and Le{\'o}n-Guzm{\'a}n, Elizabeth},
	booktitle={2016 IEEE international conferences on big data and cloud computing (BDCloud), social computing and networking (socialcom), sustainable computing and communications (sustaincom)(BDCloud-socialcom-sustaincom)},
	pages={250--257},
	year={2016},
	organization={IEEE}
}

@inproceedings{o2010tweets,
	title={From tweets to polls: Linking text sentiment to public opinion time series},
	author={O'Connor, Brendan and Balasubramanyan, Ramnath and Routledge, Bryan R and Smith, Noah A},
	booktitle={Fourth international AAAI conference on weblogs and social media},
	year={2010}
}



@inproceedings{davidov2010enhanced,
	title={Enhanced sentiment learning using twitter hashtags and smileys},
	author={Davidov, Dmitry and Tsur, Oren and Rappoport, Ari},
	booktitle={Coling 2010: Posters},
	pages={241--249},
	year={2010}
}

@article{turney2002thumbs,
	title={Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews},
	author={Turney, Peter D},
	journal={arXiv preprint cs/0212032},
	year={2002}
}



@inproceedings{yu2003towards,
	title={Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences},
	author={Yu, Hong and Hatzivassiloglou, Vasileios},
	booktitle={Proceedings of the 2003 conference on Empirical methods in natural language processing},
	pages={129--136},
	year={2003}
}

@article{wiebe1994tracking,
	title={Tracking point of view in narrative},
	author={Wiebe, Janyce M},
	journal={arXiv preprint cmp-lg/9407019},
	year={1994}
}

@inproceedings{hatzivassiloglou1997predicting,
	title={Predicting the semantic orientation of adjectives},
	author={Hatzivassiloglou, Vasileios and McKeown, Kathleen},
	booktitle={35th annual meeting of the association for computational linguistics and 8th conference of the european chapter of the association for computational linguistics},
	pages={174--181},
	year={1997}
}



@article{acheampong2021transformer,
	title={Transformer models for text-based emotion detection: a review of BERT-based approaches},
	author={Acheampong, Francisca Adoma and Nunoo-Mensah, Henry and Chen, Wenyu},
	journal={Artificial Intelligence Review},
	volume={54},
	number={8},
	pages={5789--5829},
	year={2021},
	publisher={Springer}
}

@article{gonzalez2021twilbert,
	title={TWilBert: Pre-trained deep bidirectional transformers for Spanish Twitter},
	author={Gonzalez, Jose Angel and Hurtado, Llu{\'\i}s-F and Pla, Ferran},
	journal={Neurocomputing},
	volume={426},
	pages={58--69},
	year={2021},
	publisher={Elsevier}
}

@article{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}

@article{devlin2018bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1810.04805},
	year={2018}
}


@article{canete2020spanish,
	title={Spanish pre-trained bert model and evaluation data},
	author={Canete, Jos{\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P{\'e}rez, Jorge},
	journal={Pml4dc at iclr},
	volume={2020},
	pages={1--10},
	year={2020}
}

@article{hochreiter1997long,
	title={Long short-term memory},
	author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal={Neural computation},
	volume={9},
	number={8},
	pages={1735--1780},
	year={1997},
	publisher={MIT Press}
}

@article{chung2014empirical,
	title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
	author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	journal={arXiv preprint arXiv:1412.3555},
	year={2014}
}

@article{plaza2020improved,
	title={Improved emotion recognition in Spanish social media through incorporation of lexical knowledge},
	author={Plaza-del-Arco, Flor Miriam and Mart{\'\i}n-Valdivia, M Teresa and Ure{\~n}a-L{\'o}pez, L Alfonso and Mitkov, Ruslan},
	journal={Future Generation Computer Systems},
	volume={110},
	pages={1000--1008},
	year={2020},
	publisher={Elsevier}
}

@inproceedings{sidorov2012empirical,
	title={Empirical study of machine learning based approach for opinion mining in tweets},
	author={Sidorov, Grigori and Miranda-Jim{\'e}nez, Sabino and Viveros-Jim{\'e}nez, Francisco and Gelbukh, Alexander and Castro-S{\'a}nchez, No{\'e} and Vel{\'a}squez, Francisco and D{\'\i}az-Rangel, Ismael and Su{\'a}rez-Guerra, Sergio and Trevino, Alejandro and Gordon, Juan},
	booktitle={Mexican international conference on Artificial intelligence},
	pages={1--14},
	year={2012},
	organization={Springer}
}

@inproceedings{gil2013combining,
	title={Combining machine learning techniques and natural language processing to infer emotions using Spanish Twitter corpus},
	author={Gil, Gonzalo Bl{\'a}zquez and Jes{\'u}s, Antonio Berlanga de and Lop{\'e}z, Jos{\'e} M Molina},
	booktitle={International Conference on Practical Applications of Agents and Multi-Agent Systems},
	pages={149--157},
	year={2013},
	organization={Springer}
}

@article{miller1995wordnet,
	title={WordNet: a lexical database for English},
	author={Miller, George A},
	journal={Communications of the ACM},
	volume={38},
	number={11},
	pages={39--41},
	year={1995},
	publisher={ACM New York, NY, USA}
}

@inproceedings{mohammad2018semeval,
	title={Semeval-2018 task 1: Affect in tweets},
	author={Mohammad, Saif and Bravo-Marquez, Felipe and Salameh, Mohammad and Kiritchenko, Svetlana},
	booktitle={Proceedings of the 12th international workshop on semantic evaluation},
	pages={1--17},
	year={2018}
}

  @misc{ enwiki:1109264340,
	author = "{Wikipedia contributors}",
	title = "Recurrent neural network --- {Wikipedia}{,} The Free Encyclopedia",
	year = "2022",
	howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Recurrent_neural_network&oldid=1109264340}",
	note = "[Online; accessed 26-September-2022]"
}

@article{sidorov2016construccion,
	title={Construcci{\'o}n de un corpus marcado con emociones para el an{\'a}lisis de sentimientos en Twitter en espa{\~n}ol},
	author={Sidorov, Grigori and Haro, Sof{\'\i}a Natalia Galicia and V{\'a}zquez, Vanessa Alejandra Camacho},
	journal={Revista Escritos BUAP},
	volume={1},
	number={1},
	year={2016}
}

@inproceedings{plaza2020emoevent,
	title={EmoEvent: A multilingual emotion corpus based on different events},
	author={Plaza-del-Arco, Flor Miriam and Strapparava, Carlo and Lopez, L Alfonso Urena and Mart{\'\i}n-Valdivia, M Teresa},
	booktitle={Proceedings of the 12th Language Resources and Evaluation Conference},
	pages={1492--1498},
	year={2020}
}

@article{plaza2021overview,
	title={Overview of the EmoEvalEs task on emotion detection for Spanish at IberLEF 2021},
	author={Plaza-del-Arco, Flor Miriam and Jim{\'e}nez Zafra, Salud M and Montejo R{\'a}ez, Arturo and Molina Gonz{\'a}lez, M Dolores and Ure{\~n}a L{\'o}pez, Luis Alfonso and Mart{\'\i}n Valdivia, Mar{\'\i}a Teresa},
	year={2021},
	publisher={Sociedad Espa{\~n}ola para el Procesamiento del Lenguaje Natural}
}


@inproceedings{perez-etal-2022-robertuito,
	title = "{R}o{BERT}uito: a pre-trained language model for social media text in {S}panish",
	author = "P{\'e}rez, Juan Manuel  and
	Furman, Dami{\'a}n Ariel  and
	Alonso Alemany, Laura  and
	Luque, Franco M.",
	booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
	month = jun,
	year = "2022",
	address = "Marseille, France",
	publisher = "European Language Resources Association",
	url = "https://aclanthology.org/2022.lrec-1.785",
	pages = "7235--7243",
	abstract = "Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.",
}

@misc{mromero2020electricidad-base-discriminator,
	title={Spanish Electra by Manuel Romero},
	author={Romero, Manuel},
	publisher={Hugging Face},
	journal={Hugging Face Hub},
	howpublished={\url{https://huggingface.co/mrm8488/electricidad-base-discriminator/}},
	year={2020}
}

@inproceedings{CaneteCFP2020,
	title={Spanish Pre-Trained BERT Model and Evaluation Data},
	author={Cañete, José and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and Pérez, Jorge},
	booktitle={PML4DC at ICLR 2020},
	year={2020}
}


@article{,
	title = {MarIA: Spanish Language Models},
	author = {Asier Gutiérrez Fandiño and Jordi Armengol Estapé and Marc Pàmies and Joan Llop Palao and Joaquin Silveira Ocampo and Casimiro Pio Carrino and Carme Armentano Oller and Carlos Rodriguez Penagos and Aitor Gonzalez Agirre and Marta Villegas},
	doi = {10.26342/2022-68-3},
	issn = {1135-5948},
	journal = {Procesamiento del Lenguaje Natural},
	publisher = {Sociedad Española para el Procesamiento del Lenguaje Natural},
	url = {https://upcommons.upc.edu/handle/2117/367156#.YyMTB4X9A-0.mendeley},
	volume = {68},
	year = {2022},
}


@inproceedings{perez-etal-2022-robertuito,
	title = "{R}o{BERT}uito: a pre-trained language model for social media text in {S}panish",
	author = "P{\'e}rez, Juan Manuel  and
	Furman, Dami{\'a}n Ariel  and
	Alonso Alemany, Laura  and
	Luque, Franco M.",
	booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
	month = jun,
	year = "2022",
	address = "Marseille, France",
	publisher = "European Language Resources Association",
	url = "https://aclanthology.org/2022.lrec-1.785",
	pages = "7235--7243",
	abstract = "Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.",
}


@article{BERTIN,
	author = {Javier De la Rosa and Eduardo G. Ponferrada and Manu Romero and Paulo Villegas and Pablo González de Prado Salas and María Grandury},
	title = {BERTIN: Efficient Pre-Training of a Spanish Language Model using Perplexity Sampling},
	journal = {Procesamiento del Lenguaje Natural},
	volume = {68},
	number = {0},
	year = {2022},
	keywords = {},
	abstract = {The pre-training of large language models usually requires massive amounts of resources, both in terms of computation and data. Frequently used web sources such as Common Crawl might contain enough noise to make this pretraining sub-optimal. In this work, we experiment with different sampling methods from the Spanish version of mC4, and present a novel data-centric technique which we name perplexity sampling that enables the pre-training of language models in roughly half the amount of steps and using one fifth of the data. The resulting models are comparable to the current state-of-the-art, and even achieve better results for certain tasks. Our work is proof of the versatility of Transformers, and paves the way for small teams to train their models on a limited budget.},
	issn = {1989-7553},
	url = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6403},
	pages = {13--23}
}

@misc{mromero2020electricidad-base-discriminator,
	title={Spanish Electra by Manuel Romero},
	author={Romero, Manuel},
	publisher={Hugging Face},
	journal={Hugging Face Hub},
	howpublished={\url{https://huggingface.co/mrm8488/electricidad-base-discriminator/}},
	year={2020}
}

@inproceedings{CaneteCFP2020,
	title={Spanish Pre-Trained BERT Model and Evaluation Data},
	author={Cañete, José and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and Pérez, Jorge},
	booktitle={PML4DC at ICLR 2020},
	year={2020}
}

@article{ROBERTA,
	title = {MarIA: Spanish Language Models},
	author = {Asier Gutiérrez Fandiño and Jordi Armengol Estapé and Marc Pàmies and Joan Llop Palao and Joaquin Silveira Ocampo and Casimiro Pio Carrino and Carme Armentano Oller and Carlos Rodriguez Penagos and Aitor Gonzalez Agirre and Marta Villegas},
	doi = {10.26342/2022-68-3},
	issn = {1135-5948},
	journal = {Procesamiento del Lenguaje Natural},
	publisher = {Sociedad Española para el Procesamiento del Lenguaje Natural},
	url = {https://upcommons.upc.edu/handle/2117/367156#.YyMTB4X9A-0.mendeley},
	volume = {68},
	year = {2022},
}


@inproceedings{barbieri2022xlm,
	title={Xlm-t: Multilingual language models in twitter for sentiment analysis and beyond},
	author={Barbieri, Francesco and Anke, Luis Espinosa and Camacho-Collados, Jose},
	booktitle={Proceedings of the Thirteenth Language Resources and Evaluation Conference},
	pages={258--266},
	year={2022}
}


@article{mohammad2015sentiment,
	title={Sentiment, emotion, purpose, and style in electoral tweets},
	author={Mohammad, Saif M and Zhu, Xiaodan and Kiritchenko, Svetlana and Martin, Joel},
	journal={Information Processing \& Management},
	volume={51},
	number={4},
	pages={480--499},
	year={2015},
	publisher={Elsevier}
}
